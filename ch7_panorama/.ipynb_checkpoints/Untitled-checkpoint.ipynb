{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 지역 특징 추출\n",
    "    - sift 검출기 사용하여 각각의 영상에서 지역 특징점 검출\n",
    "2) 대응점 매칭\n",
    "    - 1에서 찾은 지역 특징점 간 매칭 수행\n",
    "    - 매칭 전략\n",
    "        - 고정 임계값 사용\n",
    "        - 최근접 특징 벡터 검색 \n",
    "        - 최근접 거리 비융(*실습에서 사용)\n",
    "3) 변환 행렬(H) 추정\n",
    "    - 영상 1과 영상 2 사이의 다수의 대응점 쌍으로부터 RANSAC을 이용하여 변환 행렬 추정\n",
    "    - RANSAC은 이상점이 포함된 데이터셋에서 어떠한 모델을 예측할 때 효과적인 방법\n",
    "4) 원근 변환\n",
    "    - 추정된 변환 행렬 H를 이용하여 영상2에 원근 변환 적용\n",
    "5) 두 영상 이어붙이기\n",
    "    - 파노라마 결과 영상 = 영상1 + 변환된 영상2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "## 테스트할 이미지\n",
    "img_1 = cv2.imread('left.jpg')\n",
    "img_1 = cv2.resize(img_1, None, fx=0.5, fy=0.5)\n",
    "gray_1 = cv2.cvtColor(img_1,cv2.COLOR_BGR2GRAY)\n",
    "img_2 = cv2.imread('right.jpg')\n",
    "img_2 = cv2.resize(img_2, None, fx=0.5, fy=0.5)\n",
    "gray_2 = cv2.cvtColor(img_2,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sift 이용해서 지역 특징 추출\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp_1, des_1 = sift.detectAndCompute(gray_1,None) #KeyPoint(kp), 특징기술자(des) 뽑아줌 \n",
    "kp_2, des_2 = sift.detectAndCompute(gray_2,None)\n",
    "\n",
    "# 2.1 두 영상의 지역 특징 간 거리 계산\n",
    "bf = cv2.BFMatcher()\n",
    "matches =bf.knnMatch(queryDescriptors = des_1, trainDescriptors = des_2,k=2) # 특징점 하나당 거리가 가까운 상위 K개의 대응점 찾음\n",
    "\n",
    "# 2.2 '최근접 거리 비융' 매칭 전략(ratio testing) 을 사용하여 대응점 쌍 생성\n",
    "ratio = 0.7\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < n.distance * ratio:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "## optional) SIFT 특징점 매칭 결과 확인하기\n",
    "print(len(good))\n",
    "matched = cv2.drawMatches(img1 = img_1, \n",
    "                          keypoints1 = kp_1, \n",
    "                          img2 = img_2, \n",
    "                          keypoints2 = kp_2, \n",
    "                          matches1to2 = good[:20],   # 첫 20개 매칭쌍만 시각화\n",
    "                          outImg = None, \n",
    "                          flags = 2)\n",
    "\n",
    "#cv2.imshow('matching result', matched)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANSAC을 이용한 변환 행렬 H 추정\n",
    "\n",
    "## 참고. cv2.getPerspectiveTransform()와 cv2.findHomography()의 차이\n",
    "##   cv2.getPerspectiveTransform() - 4개의 대응점 쌍을 입력하면 변환 행렬을 반환해줌\n",
    "##   cv2.findHomography() - 4개 이상의 대응점 쌍을 입력하면 가장 대응점들을 가장 잘 만족하는 변환 행렬을 반환해줌\n",
    "\n",
    "if len(good) >= 4:   # 4개 이상의 대응점이 존재해야 원근 변환 행렬을 추정 가능\n",
    "    src_pts = np.float32([ kp_2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp_1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    H, _ = cv2.findHomography(srcPoints = src_pts, \n",
    "                              dstPoints = dst_pts, \n",
    "                              method = cv2.RANSAC, \n",
    "                              ransacReprojThreshold = 5) \n",
    "    \n",
    "else:\n",
    "    raise AssertionError(\"Can't find enough keypoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) 원근 변환 적용\n",
    "## 변환 행렬 H를 사용하여 변환 수행\n",
    "## https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective\n",
    "## cv2.warpPerspective(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]) → dst\n",
    "## M -> H\n",
    "## 오른쪽 영상 변환해서 왼쪽 사진에 이어 붙이기\n",
    "#height1, width1 = img_1.shape[:2]\n",
    "#height2, width2 = img_2.shape[:2]\n",
    "#transformed = cv2.warpPerspective(img_2,H,(width1+width2,height1))\n",
    "\n",
    "res = cv2.warpPerspective(src = img_2,\n",
    "                          M = H,\n",
    "                          dsize = (img_1.shape[1]+img_2.shape[1], img_1.shape[0]))\n",
    "\n",
    "# cv2.imshow('right image', img_r)\n",
    "# cv2.imshow('transformed right image', res)\n",
    "# cv2.imshow('left image', img_l)\n",
    "\n",
    "## 5) 두 영상 이어붙이기\n",
    "#res = np.hstack((img_1,transformed))\n",
    "res[0:img_1.shape[0], 0:img_1.shape[1]] = img_1 # 왼쪽 영상을 변환된 오른쪽 영상에 오버랩함\n",
    "cv2.imshow('panorama', res)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
